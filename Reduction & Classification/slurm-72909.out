/home/Student/s4823870/miniconda3/envs/conda-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Random Seed:  999
Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace=True)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace=True)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace=True)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace=True)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace=True)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace=True)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace=True)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (12): Sigmoid()
  )
)
Starting Training Loop...
[0/5][0/724]	Loss_D: 1.7611	Loss_G: 5.2459	D(x): 0.5348	D(G(z)): 0.5840 / 0.0080
[0/5][50/724]	Loss_D: 0.2736	Loss_G: 29.1527	D(x): 0.8642	D(G(z)): 0.0000 / 0.0000
[0/5][100/724]	Loss_D: 0.3213	Loss_G: 8.2342	D(x): 0.8920	D(G(z)): 0.1055 / 0.0006
[0/5][150/724]	Loss_D: 0.4640	Loss_G: 5.0257	D(x): 0.7918	D(G(z)): 0.1260 / 0.0133
[0/5][200/724]	Loss_D: 0.8983	Loss_G: 6.8514	D(x): 0.9216	D(G(z)): 0.4934 / 0.0029
[0/5][250/724]	Loss_D: 0.3999	Loss_G: 3.0803	D(x): 0.8835	D(G(z)): 0.1734 / 0.0813
[0/5][300/724]	Loss_D: 0.4569	Loss_G: 3.6666	D(x): 0.8554	D(G(z)): 0.2102 / 0.0462
[0/5][350/724]	Loss_D: 0.4337	Loss_G: 5.2288	D(x): 0.8825	D(G(z)): 0.2183 / 0.0111
[0/5][400/724]	Loss_D: 0.5469	Loss_G: 6.2233	D(x): 0.9213	D(G(z)): 0.3043 / 0.0061
[0/5][450/724]	Loss_D: 0.9232	Loss_G: 7.4747	D(x): 0.9578	D(G(z)): 0.4971 / 0.0033
[0/5][500/724]	Loss_D: 0.5115	Loss_G: 3.7232	D(x): 0.7618	D(G(z)): 0.1123 / 0.0398
[0/5][550/724]	Loss_D: 0.8438	Loss_G: 3.5287	D(x): 0.5814	D(G(z)): 0.0242 / 0.0622
[0/5][600/724]	Loss_D: 0.5815	Loss_G: 6.9918	D(x): 0.9321	D(G(z)): 0.3275 / 0.0033
[0/5][650/724]	Loss_D: 0.9778	Loss_G: 3.1248	D(x): 0.5038	D(G(z)): 0.0061 / 0.0917
[0/5][700/724]	Loss_D: 0.9362	Loss_G: 9.3069	D(x): 0.9501	D(G(z)): 0.4925 / 0.0004
[1/5][0/724]	Loss_D: 0.5329	Loss_G: 5.1009	D(x): 0.9146	D(G(z)): 0.2954 / 0.0130
[1/5][50/724]	Loss_D: 0.4634	Loss_G: 5.2505	D(x): 0.8442	D(G(z)): 0.2192 / 0.0089
[1/5][100/724]	Loss_D: 0.5030	Loss_G: 3.5928	D(x): 0.7472	D(G(z)): 0.0745 / 0.0454
[1/5][150/724]	Loss_D: 0.9528	Loss_G: 2.8497	D(x): 0.5342	D(G(z)): 0.0206 / 0.1037
[1/5][200/724]	Loss_D: 0.4582	Loss_G: 4.2092	D(x): 0.7794	D(G(z)): 0.1146 / 0.0345
[1/5][250/724]	Loss_D: 0.4647	Loss_G: 4.8299	D(x): 0.9199	D(G(z)): 0.2777 / 0.0180
[1/5][300/724]	Loss_D: 0.6206	Loss_G: 6.1235	D(x): 0.8670	D(G(z)): 0.3202 / 0.0045
[1/5][350/724]	Loss_D: 0.7327	Loss_G: 3.5590	D(x): 0.5911	D(G(z)): 0.0143 / 0.0640
[1/5][400/724]	Loss_D: 0.8672	Loss_G: 5.3107	D(x): 0.9248	D(G(z)): 0.4567 / 0.0147
[1/5][450/724]	Loss_D: 0.1499	Loss_G: 3.6873	D(x): 0.9192	D(G(z)): 0.0453 / 0.0693
[1/5][500/724]	Loss_D: 0.5779	Loss_G: 3.8352	D(x): 0.8397	D(G(z)): 0.2775 / 0.0374
[1/5][550/724]	Loss_D: 0.1879	Loss_G: 5.0991	D(x): 0.9275	D(G(z)): 0.0904 / 0.0145
[1/5][600/724]	Loss_D: 0.7567	Loss_G: 6.6098	D(x): 0.9308	D(G(z)): 0.4245 / 0.0031
[1/5][650/724]	Loss_D: 0.3478	Loss_G: 4.2824	D(x): 0.8617	D(G(z)): 0.1477 / 0.0224
[1/5][700/724]	Loss_D: 2.3456	Loss_G: 5.7890	D(x): 0.9558	D(G(z)): 0.8109 / 0.0079
[2/5][0/724]	Loss_D: 0.3398	Loss_G: 4.2679	D(x): 0.8806	D(G(z)): 0.1629 / 0.0247
[2/5][50/724]	Loss_D: 0.4869	Loss_G: 2.9872	D(x): 0.7727	D(G(z)): 0.1419 / 0.0763
[2/5][100/724]	Loss_D: 0.9626	Loss_G: 0.6403	D(x): 0.5101	D(G(z)): 0.0364 / 0.6047
[2/5][150/724]	Loss_D: 0.8913	Loss_G: 6.9663	D(x): 0.9379	D(G(z)): 0.4833 / 0.0022
[2/5][200/724]	Loss_D: 1.8105	Loss_G: 6.8214	D(x): 0.9780	D(G(z)): 0.7236 / 0.0037
[2/5][250/724]	Loss_D: 0.5334	Loss_G: 2.4248	D(x): 0.6984	D(G(z)): 0.0623 / 0.1351
[2/5][300/724]	Loss_D: 0.3940	Loss_G: 3.3004	D(x): 0.7990	D(G(z)): 0.1023 / 0.0583
[2/5][350/724]	Loss_D: 0.4185	Loss_G: 3.2902	D(x): 0.7602	D(G(z)): 0.0662 / 0.0615
[2/5][400/724]	Loss_D: 1.9150	Loss_G: 0.5517	D(x): 0.2762	D(G(z)): 0.0081 / 0.6352
[2/5][450/724]	Loss_D: 0.9220	Loss_G: 5.5257	D(x): 0.8849	D(G(z)): 0.4721 / 0.0069
[2/5][500/724]	Loss_D: 0.4712	Loss_G: 2.9592	D(x): 0.7070	D(G(z)): 0.0566 / 0.0838
[2/5][550/724]	Loss_D: 0.6073	Loss_G: 4.7313	D(x): 0.9289	D(G(z)): 0.3607 / 0.0148
[2/5][600/724]	Loss_D: 0.3085	Loss_G: 3.5239	D(x): 0.8577	D(G(z)): 0.1212 / 0.0446
[2/5][650/724]	Loss_D: 0.7568	Loss_G: 2.2170	D(x): 0.5740	D(G(z)): 0.0380 / 0.1930
[2/5][700/724]	Loss_D: 0.4079	Loss_G: 3.5985	D(x): 0.8223	D(G(z)): 0.1591 / 0.0444
[3/5][0/724]	Loss_D: 0.4257	Loss_G: 3.6666	D(x): 0.8555	D(G(z)): 0.2059 / 0.0392
[3/5][50/724]	Loss_D: 0.8908	Loss_G: 6.0174	D(x): 0.9051	D(G(z)): 0.4747 / 0.0063
[3/5][100/724]	Loss_D: 0.4651	Loss_G: 4.0765	D(x): 0.8538	D(G(z)): 0.2245 / 0.0266
[3/5][150/724]	Loss_D: 0.4851	Loss_G: 3.5194	D(x): 0.8484	D(G(z)): 0.2368 / 0.0438
[3/5][200/724]	Loss_D: 0.5556	Loss_G: 2.7881	D(x): 0.7604	D(G(z)): 0.1639 / 0.0967
[3/5][250/724]	Loss_D: 0.6097	Loss_G: 2.2545	D(x): 0.6361	D(G(z)): 0.0686 / 0.1488
[3/5][300/724]	Loss_D: 1.2665	Loss_G: 6.2836	D(x): 0.8873	D(G(z)): 0.5971 / 0.0037
[3/5][350/724]	Loss_D: 0.5994	Loss_G: 5.1727	D(x): 0.8686	D(G(z)): 0.3185 / 0.0102
[3/5][400/724]	Loss_D: 0.4151	Loss_G: 3.0752	D(x): 0.8090	D(G(z)): 0.1485 / 0.0648
[3/5][450/724]	Loss_D: 0.7060	Loss_G: 2.8394	D(x): 0.6805	D(G(z)): 0.2154 / 0.0874
[3/5][500/724]	Loss_D: 0.4089	Loss_G: 2.8311	D(x): 0.7822	D(G(z)): 0.1146 / 0.0797
[3/5][550/724]	Loss_D: 0.8400	Loss_G: 1.9590	D(x): 0.6300	D(G(z)): 0.1893 / 0.2044
[3/5][600/724]	Loss_D: 0.4668	Loss_G: 3.4572	D(x): 0.9011	D(G(z)): 0.2661 / 0.0495
[3/5][650/724]	Loss_D: 0.8965	Loss_G: 4.7671	D(x): 0.9395	D(G(z)): 0.5060 / 0.0129
[3/5][700/724]	Loss_D: 0.7238	Loss_G: 1.9076	D(x): 0.5812	D(G(z)): 0.0543 / 0.1949
/home/Student/s4823870/miniconda3/envs/conda-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
[4/5][0/724]	Loss_D: 0.8818	Loss_G: 1.3754	D(x): 0.5321	D(G(z)): 0.0755 / 0.3111
[4/5][50/724]	Loss_D: 2.1612	Loss_G: 7.0716	D(x): 0.9691	D(G(z)): 0.8100 / 0.0019
[4/5][100/724]	Loss_D: 0.5183	Loss_G: 3.4252	D(x): 0.8734	D(G(z)): 0.2801 / 0.0437
[4/5][150/724]	Loss_D: 0.4486	Loss_G: 2.4824	D(x): 0.8244	D(G(z)): 0.1949 / 0.1046
[4/5][200/724]	Loss_D: 0.5439	Loss_G: 3.9339	D(x): 0.9011	D(G(z)): 0.3087 / 0.0301
[4/5][250/724]	Loss_D: 0.5883	Loss_G: 3.6317	D(x): 0.8741	D(G(z)): 0.3304 / 0.0372
[4/5][300/724]	Loss_D: 0.5941	Loss_G: 2.8056	D(x): 0.8118	D(G(z)): 0.2809 / 0.0838
[4/5][350/724]	Loss_D: 0.7780	Loss_G: 4.4809	D(x): 0.9303	D(G(z)): 0.4582 / 0.0154
[4/5][400/724]	Loss_D: 0.6237	Loss_G: 3.4983	D(x): 0.9279	D(G(z)): 0.3747 / 0.0480
[4/5][450/724]	Loss_D: 0.7882	Loss_G: 2.8496	D(x): 0.7659	D(G(z)): 0.3529 / 0.0785
[4/5][500/724]	Loss_D: 1.1410	Loss_G: 0.8053	D(x): 0.3812	D(G(z)): 0.0270 / 0.4959
[4/5][550/724]	Loss_D: 0.5726	Loss_G: 2.6011	D(x): 0.8119	D(G(z)): 0.2735 / 0.0960
[4/5][600/724]	Loss_D: 2.1706	Loss_G: 5.6119	D(x): 0.9799	D(G(z)): 0.8050 / 0.0105
[4/5][650/724]	Loss_D: 0.4662	Loss_G: 2.6528	D(x): 0.8082	D(G(z)): 0.1886 / 0.0911
[4/5][700/724]	Loss_D: 0.9961	Loss_G: 1.3580	D(x): 0.4678	D(G(z)): 0.0799 / 0.3141
